args: !!python/object:argparse.Namespace
  architecture: Money_former_MLA_DINT_cog_attn_MTP
  batch_size: 32
  bias: true
  config: ./experiment_configs/MTP_triplicate.json
  d_ff: 64
  d_model: 64
  dataset: Money
  dropout: 0.1
  dtype: fp32
  experiment_notes: all good global znorm features so far (vpt_chlo, ppo_chlov, clv,
    vix_chlo, copper_chlov) , ema's of chlov returns, global znorm volatility (ret,
    from c), returns, time data, bias, global znorm, sus in architecture (but it gives
    better results, so)
  extra_descriptor: ''
  folder_name: post_causality_fix/tests/MTP_preds/3bee_aapl_gspc/
  head_dim: 16
  indices_to_predict:
  - 1
  - 2
  - 3
  input_features: 73
  kv_compression_dim: 24
  lr: 0.001
  lr_mult: 0.5
  nhead: 4
  normalization_means:
  - - 0.0011501869885250926
    - 0.0002192855899920687
  - - 0.0010750514920800924
    - 0.0001902884541777894
  - - 0.0011410083388909698
    - 0.00021137611474841833
  - - 0.0011758849723264575
    - 0.0002184742916142568
  - - 0.06895440816879272
    - 0.017772924154996872
  - - 0.019457325339317322
    - 0.009385625831782818
  - - 0.02034885808825493
    - 0.009685884229838848
  - - 0.020972635596990585
    - 0.009847075678408146
  - - 0.021701140329241753
    - 0.010076923295855522
  - - 0.0011550590861588717
    - 0.0002212333056377247
  - - 0.001148912007920444
    - 0.0002198849688284099
  - - 0.0011377260088920593
    - 0.0002163458411814645
  - - 0.0011164348106831312
    - 0.0002098797122016549
  - - 0.0010736591648310423
    - 0.0001915360480779782
  - - 0.0010675778612494469
    - 0.00019010185496881604
  - - 0.001057085464708507
    - 0.00018659309716895223
  - - 0.0010375361889600754
    - 0.0001803687191568315
  - - 0.0011334980372339487
    - 0.00021104807092342526
  - - 0.001125036389566958
    - 0.0002092554932460189
  - - 0.001112423138692975
    - 0.00020531447080429643
  - - 0.0010904902592301369
    - 0.0001983709807973355
  - - 0.0011668726801872253
    - 0.0002170285297324881
  - - 0.0011593436356633902
    - 0.0002147932827938348
  - - 0.0011481621768325567
    - 0.00021087235654704273
  - - 0.0011284538777545094
    - 0.0002045010041911155
  - - 0.06920605897903442
    - 0.01780116930603981
  - - 0.06929291039705276
    - 0.017812613397836685
  - - 0.0693691223859787
    - 0.01782795414328575
  - - 0.06934664398431778
    - 0.0178225114941597
  - - -1130816384.0
    - 381528544.0
  - - -350457472.0
    - -104698976.0
  - - -3006495232.0
    - -1408664448.0
  - - -1512589440.0
    - -1049073024.0
  - - 0.0018001118442043662
    - 0.00033496570540592074
  - - 0.0034593569580465555
    - 0.0006454413523897529
  - - 0.005649815779179335
    - 0.001030595856718719
  - - 0.001799944555386901
    - 0.0003374800144229084
  - - 0.0034590058494359255
    - 0.0006476874114014208
  - - 0.005635315086692572
    - 0.001030224608257413
  - - 0.00180541118606925
    - 0.00033202808117493987
  - - 0.0034706376027315855
    - 0.0006390662747435272
  - - 0.005673517473042011
    - 0.0010266132885590196
  - - 0.0018080830341205
    - 0.0003338311507832259
  - - 0.003476635320112109
    - 0.0006428937194868922
  - - 0.005668214056640863
    - 0.0010269494960084558
  - - -0.009834511205554008
    - -0.0013830208918079734
  - - -0.00936779286712408
    - -0.00039023824501782656
  - - -0.0128901032730937
    - -0.0002603043685667217
  - - 0.04082256183028221
    - 0.10414284467697144
  - - 19.3928165435791
    - 19.3928165435791
  - - 20.3253116607666
    - 20.3253116607666
  - - 18.686904907226562
    - 18.686904907226562
  - - 19.51909065246582
    - 19.51909065246582
  - - 2.4868054389953613
    - 2.4868054389953613
  - - 2.5050551891326904
    - 2.5050551891326904
  - - 2.465613603591919
    - 2.465613603591919
  - - 2.4861366748809814
    - 2.4861366748809814
  - - 1216.218994140625
    - 1216.218994140625
  normalization_stds:
  - - 0.024003302678465843
    - 0.01162923313677311
  - - 0.020968643948435783
    - 0.008828666061162949
  - - 0.022897690534591675
    - 0.010568142868578434
  - - 0.02477148361504078
    - 0.01116415299475193
  - - 0.47613394260406494
    - 0.20709605515003204
  - - 0.014403990469872952
    - 0.007290278561413288
  - - 0.012886453419923782
    - 0.006739337928593159
  - - 0.011865343898534775
    - 0.006364030297845602
  - - 0.011401033960282803
    - 0.005907234735786915
  - - 0.010609103366732597
    - 0.004782244563102722
  - - 0.007587395142763853
    - 0.0032502005342394114
  - - 0.005500501953065395
    - 0.002221701666712761
  - - 0.003669766942039132
    - 0.0013603447005152702
  - - 0.01007502619177103
    - 0.004147850442677736
  - - 0.007394978776574135
    - 0.0029337871819734573
  - - 0.005464829970151186
    - 0.002072205301374197
  - - 0.0037077711895108223
    - 0.001317203976213932
  - - 0.01067829504609108
    - 0.004860825836658478
  - - 0.0077031999826431274
    - 0.0033673467114567757
  - - 0.005603672470897436
    - 0.00232044979929924
  - - 0.0037385886535048485
    - 0.0014217814896255732
  - - 0.010698918253183365
    - 0.004710451699793339
  - - 0.007660399656742811
    - 0.0032246350310742855
  - - 0.005567485932260752
    - 0.0022150224540382624
  - - 0.003715621307492256
    - 0.0013620633399114013
  - - 0.17436565458774567
    - 0.07040907442569733
  - - 0.1122630313038826
    - 0.04344061389565468
  - - 0.07477875798940659
    - 0.02704288810491562
  - - 0.047021809965372086
    - 0.015142877586185932
  - - 1488728192.0
    - 930542976.0
  - - 1770724352.0
    - 941473408.0
  - - 628467520.0
    - 1034157120.0
  - - 1114824576.0
    - 993525376.0
  - - 0.01744726486504078
    - 0.006553092040121555
  - - 0.025858212262392044
    - 0.009068775922060013
  - - 0.040576450526714325
    - 0.014918538741767406
  - - 0.017020102590322495
    - 0.006123862229287624
  - - 0.025442861020565033
    - 0.008650647476315498
  - - 0.039888326078653336
    - 0.014140857383608818
  - - 0.017645740881562233
    - 0.006920975167304277
  - - 0.0260710921138525
    - 0.009522336535155773
  - - 0.040935907512903214
    - 0.01568385772407055
  - - 0.017233213409781456
    - 0.006548942532390356
  - - 0.02564467489719391
    - 0.009076894260942936
  - - 0.04026709496974945
    - 0.0149239432066679
  - - 0.09066320210695267
    - 0.04522532597184181
  - - 0.08584611862897873
    - 0.0416116826236248
  - - 0.17202986776828766
    - 0.08132895827293396
  - - 0.6130127310752869
    - 0.6630622744560242
  - - 8.577179908752441
    - 8.577179908752441
  - - 9.105085372924805
    - 9.105085372924805
  - - 8.120172500610352
    - 8.120172500610352
  - - 8.641040802001953
    - 8.641040802001953
  - - 1.0547841787338257
    - 1.0547841787338257
  - - 1.0632758140563965
    - 1.0632758140563965
  - - 1.0449466705322266
    - 1.0449466705322266
  - - 1.055155873298645
    - 1.055155873298645
  - - 4957.81884765625
    - 4957.81884765625
  num_layers: 4
  num_params: 196829
  orthograd: false
  predict_gaussian: false
  q_compression_dim: 48
  qk_rope_dim: 4
  seq_len: 16
  t_0: 4000
  t_mult: 1.5
  t_total: 5000
  tickers:
  - AAPL
  - ^GSPC
  type: ''
  warmup_steps: 1000
batch_size: 32
learning_rate: 0.001
lr_mult: 0.5
t_0: 5000
t_mult: 1.5
warmup_steps: 1000
