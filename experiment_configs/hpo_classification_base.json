{
    "architecture": "Money_former_MLA_DINT_cog_attn_MTP",
    "d_model": 64,
    "nhead": 2,
    "head_dim": 32,
    "qk_rope_dim": 16,
    "dropout": 0.25,
    "num_layers": 4,
    "d_ff": 64,
    "kv_compression_dim": 16,
    "q_compression_dim": 32,
    "seq_len": 3,
    "batch_size": 32,
    "bias": false,
    "indices_to_predict": [1, 2],
    "unique_inputs_ratio": [0, 1],
    "input_features": 0,
    "tickers": ["AAPL","^GSPC", "MSFT", "INTC", "NVDA", "AMZN"],
    "prediction_type": "classification",
    "lr": 10e-4,
    "muon_lr": 0.005,
    "warmup_steps": 1000,
    "t_0": 4000,
    "t_mult": 1.5,
    "lr_mult": 0.5,
    "t_total": 5000,
    "optimizer": "muon",
    "aux_optimizer": "adamw",
    "n_noisy_copies": 0,
    "noise_factor": 0.05,
    "num_classes": 3,
    "classification_threshold": 0.01,
    "include_sep_in_loss": false,
    "use_global_seperator": true,
    "dtype":"fp32",
    "dataset":"Money",
    "type": "",
    "extra_descriptor":"",
    "experiment_notes": "normed feats, global sep, full attn cuz we cool like that, top fraction of 'optimised' feats (hopefully actually true), c is 10x more important in loss, ticker 0 is 1x more important in loss",
    "folder_name": "testing/fixed_mixing/test_class_seq_3_2_opt_feats_02_no_bias_weighted_loss_10/"
}