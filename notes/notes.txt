MLA
apparently grants a speed up based on compression ratio (but attention isnt that costly at my levels of seq len)
do not know yet how well it compares
need to run it on wikitext for baseline

orthograd
seems to have no negative or positive effect - gonna keep it just in case
(choise of loss fn also seems insignificant)

nGPT
the simpler version seems like it starts converging faster, but ultimately they both perform ~the same
- gonna keep simpler ver. for now, cuz its faster (~4%)
(base attn matrix is very different at the individual head level without qk norm - spotty)

DINT
v1 and v2 perform similarly on the tinyshakespeare dataset
maybe v2 is better for bigger datasets? (needs some testing)

DIFF
for some reason a disapointment, at least its better than LLaMa, at least a little bit

DINT + nGPT
the combo works?
seems like it has a middleground of both training properties, so likely does synergize, 
but will see how it does on more complex tasks
(need to test all versions at least once) (no qk norm, split norm, standard qk norm, etc.)
seemingly becomes invariant to when it starts converging in train acc


General notes:
it seems that all models except DINT containing ones perform better on validation when
the dataset is more complex. (could be that DINT removes noise from attn which usually promotes memorization/overfitting)
at <10M params, fp32 is faster, or equivalent

TODO
take things from here (https://github.com/evintunador/gpt-lab)
need to rewrite most of the code to work by passing args namespace
DINT + nGPT + MLA??? (could be lit) (gotta remember to check which parts would need to be cos normed though)

true fp16 training (basically ground up rewrite)
flash attn 2 (fails compile)

add code for comparing different modules speeds (two ways to write the same thing)
check whether ff layer can have compression in the middle (similar to MLA?) could be a good speed up if yes 


Money former
(from training on one stock and 3 points in time for prediction)
NLL (gaussian) is a bad idea, horribly overfits, even with orthograd
SGD is just worse than Adam

Returns are the way to go!!! (much better than z normed prices)
finally beats persistence model (kind of an achievement?)
With returns, seems to learns everything within the first ~300 steps (makes sense, not really the most complicated dataset)

means and stds dont seem bad? to feed as additonal input features, as the acc went down, but it seems more stable?

#########current flaws, that need fixing asap##########
returns are not relative, but absolute (should be relative)
targets need to be calculated differently, currently only the return in regards to day-1, but should be relative to the day that gets passed into model



TODOs
add more data to time_series_data
convert money_former to be able to take multiple sequences (hella custom embeddings)




##########   hustorical mistakes ############

breaking causality ~till nGPT (used the future to predict the present)
doing RoPE wrong ~till MLA (using wrong dim size to get seq len and multiply, basically was all just multiplied by the same rotation vec)

was multiplying input embeddings by sqrt(d_model) instead of dividing them, fml

was calculating returns for volume