MLA
apparently grants a speed up based on compression ratio (but attention isnt that costly at my levels of seq len)
do not know yet how well it compares
need to run it on wikitext for baseline

orthograd
seems to have no negative or positive effect - gonna keep it just in case
(choise of loss fn also seems insignificant)

nGPT
the simpler version seems like it starts converging faster, but ultimately they both perform ~the same
- gonna keep simpler ver. for now, cuz its faster (~4%)
(base attn matrix is very different at the individual head level without qk norm - spotty)

DINT
v1 and v2 perform similarly on the tinyshakespeare dataset
maybe v2 is better for bigger datasets? (needs some testing)

DIFF
for some reason a disapointment, at least its better than LLaMa, at least a little bit

DINT + nGPT
the combo works?
seems like it has a middleground of both training properties, so likely does synergize, 
but will see how it does on more complex tasks
(need to test all versions at least once) (no qk norm, split norm, standard qk norm, etc.)
seemingly becomes invariant to when it starts converging in train acc


General notes:
it seems that all models except DINT containing ones perform better on validation when
the dataset is more complex. (could be that DINT removes noise from attn which usually promotes memorization/overfitting)

TODO
need to rewrite most of the code to work by passing args namespace
DINT + nGPT + MLA??? (could be lit) (gotta remember to check which parts would need to be cos normed though)
try hand at predicting markets (trading view for data?)
