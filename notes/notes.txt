orthograd
seems to have no negative or positive effect - gonna keep it just in case
(choise of loss fn also seems insignificant)

nGPT
the simpler version seems like it starts converging faster, but ultimately they both perform ~the same
- gonna keep simpler ver. for now, cuz its faster (~4%)

DINT
v1 and v2 perform similarly on the tinyshakespeare dataset
maybe v2 is better for bigger datasets? (needs some testing)

DIFF
for some reason a disapointment, at least its better than LLaMa, at least a little bit

DINT + nGPT
the combo works?
seems like it has a middleground of both training properties, so likely does synergize, 
but will see how it does on more complex tasks
(need to test all versions at least once) (no qk norm, split norm, standard qk norm, etc.)
seemingly becomes invariant to when it starts converging in train acc



