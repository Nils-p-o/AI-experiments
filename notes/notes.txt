MLA
apparently grants a speed up based on compression ratio (but attention isnt that costly at my levels of seq len)
do not know yet how well it compares
need to run it on wikitext for baseline

orthograd
seems to have no negative or positive effect - gonna keep it just in case
(choise of loss fn also seems insignificant)

nGPT
the simpler version seems like it starts converging faster, but ultimately they both perform ~the same
- gonna keep simpler ver. for now, cuz its faster (~4%)
(base attn matrix is very different at the individual head level without qk norm - spotty)

DINT
v1 and v2 perform similarly on the tinyshakespeare dataset
maybe v2 is better for bigger datasets? (needs some testing)

DIFF
for some reason a disapointment, at least its better than LLaMa, at least a little bit

DINT + nGPT
the combo works?
seems like it has a middleground of both training properties, so likely does synergize, 
but will see how it does on more complex tasks
(need to test all versions at least once) (no qk norm, split norm, standard qk norm, etc.)
seemingly becomes invariant to when it starts converging in train acc


General notes:
it seems that all models except DINT containing ones perform better on validation when
the dataset is more complex. (could be that DINT removes noise from attn which usually promotes memorization/overfitting)
at <10M params, fp32 is faster, or equivalent

TODO
need to rewrite most of the code to work by passing args namespace
DINT + nGPT + MLA??? (could be lit) (gotta remember to check which parts would need to be cos normed though)
try hand at predicting markets (trading view for data?)

true fp16 training (basically ground up rewrite)
flash attn 2 (fails compile)

add code for comparing different modules speeds (two ways to write the same thing)
check whether ff layer can have compression in the middle (similar to MLA?) could be a good speed up if yes 



##########   hustorical mistakes ############

breaking causality ~till nGPT (used the future to predict the present)
doing RoPE wrong ~till MLA (using wrong dim size to get seq len and multiply, basically was all just multiplied by the same rotation vec)

was multiplying input embeddings by sqrt(d_model) instead of dividing them, fml