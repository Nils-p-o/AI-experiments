similar to notes, but just for money former concenpt

################################################
current notes:
just because model performs worse during training trading strat metrics, does not mean the model is worse potential wise,
maybe even better with a good probability thresh

ALL of my trading metric based conclusions are void -- the trading strategy function was wrong, it was doing some spooky shit, which led
to leverage in the simulation
--nvm, barely a fucking difference in actual metrics

TODO
seems like ticker weighing may be useful in ensemble creation? (not quite stat sig though)
ensemble model is a great success, but i should redo it with better train/val/test splits, to make sure, because currently i am relying on some bias


maybe global znorm of feats could be bad? (might need to do procedural znorm, like only using the stats from up to t to norm  t+1)

# TODOs

tiers: ~(easiest to hardest)
1. loss weighing / unique inputs/outputs
2. feats
3. encoder-decoder aux
4. BERT semantic


indexes should be cut, same as data

then,
analyze everything (just start from the beginning, and build up)
figure out whether class or regression or nll is better
figure out why feats all of a sudden dont matter
what loss weighing is best
...

dunno if original loss weighted, or balanced loss + linear cost is better

maybe revisit unique inputs (for aux seq specifically)

think about why gpu usage is so low on old pc (too much cpu utilization somewhere?)

local feats do nothing, just a waste of space/parameters
rework aux feats:
-split between those that have vol and those that dont?
-alter how data gets saved?
-shared time feat encoding (for all types of feats)
-noisy data gen (currently only works with main data)
-test more feats


check trained model input layer weights to see which features are important

back to regression?
rethink f1 to be weighted

maybe do way more specialized ver of model? (unique ffn, input,outputs heads)?

why is the simple 1.5x weighing better than the per feat/sequence weighing?
(maybe because no unique output heads?)

just do every test out there - 
-what metrics should i trust? (combined/weighted f1? or calmar?)

feats
hlo based on c?

to be implemented in new best model:
muon + muP (kinda already done)
aux seqs -- just need to find how much helps

more stocks? - would help with data, and thus regularization, + higher theoretical profit ceiling/lower risk?

more reguralization - dont know how, but currently the limiting factor, as bigger =/= better (i think, needs stat test)

maybes:
cce weighing  -- (weighing up/down by 1.5x as much)
custom loss?

bigger batches? - dunno, kinda a dead end, i think
more data, somehow?
noise -- stat test

investigate hpo for linear decay and constant lr -- might be better

needs a bit of work:
bert Inclusion
diffusion lm
nGPT - again?
encoder-decoder aux (maybe all causal?)



find optimum for custom loss (both ver)
custom custom loss
find optimum for aux seqs (value + which help)

maybe i should just do class imbalance weighing in cce? (that could be why the original cce weighing worked?)
not really?


class imbalance + custom loss exp - can definitely be tuned to reign in worst case errors

deffo improvements:
cce weighing - maybe 1.5 is best (makes sense, thats ~ class imbalance)
--better calmar, return, f1 --worse loss --higher trading days
custom loss function -- no stat diff between linear and exp, i will just assume exp is better (calmar is non stat higher, ig)
--better loss, calmar
aux seqs -- will be more data + i can already see it helps with reguralization,
--better calmar, return, --higher trading days

with current weighing, cce weighing and custom loss dont work better together (for now, staying with cce weighing)

maybe (inconclusive):
LoRA ff ~ the same performance? but should scale better for speed, so proposal for big model ver
global data-- idk loss is worse, but all else is ~ the same
local simple?
feats loss weighing --- not really great rn

higher causal seq_len - shit
causal - seems the same as "full" attn, but maybe i should use it when i scale up, because it is definitely more regularized

second mask fill in custom mha - seems like nothing, maybe worse loss?
scaling swiglu - also nothing

from scaled up ver:
more sequences allows for more model size
local feats do not improve loss
seq_len does not really matter (so sticking with 2)
causal is not better - just worse performance and slower convergence

engineered feats do not really affect performance, when super many stocks
- however they do prevent overfitting a bit/keep acc improving





Money former
V2:
2 signifies using multiple time series sequences (the vision), might be merged into normal file, if performs better

overnight trading to fill gaps in info?

threshold plays a big part in accuracy

for iteration speed purposes currently running models with ~4 attn heads, even though more seems to be better in general (2-3x faster than 20 or 30 heads)
"AAPL","^GSPC", "MSFT", "INTC", "NVDA", "AMZN"
, "^DJI", "^NDX", "ORCL", "CSCO", "IBM", "TXN", "KO", "PG", "PEP", "WMT", "JNJ", "PFE", "MRK", "JPM", "BA", "CAT", "MMM", "RTX", "XOM", "CVX"

current optimum:
d_model = 128
num_layers = 6
nhead = 8
head_dim = 32
rope_dim = 8
d_ff = 128
kv_com_dim = 48
q_com_dim = 64
seq_len = 8

6 tickers

info baseline: (3bee6)
all global znorm (vpt_chlo, ppo_chlov, clv, vix_chlo, copper_chlov, emas of chlov returns, vol of c returns)
tickers: ["AAPL","^GSPC", "MSFT", "INTC", "NVDA", "AMZN"]


V1:
(from training on one stock and 3 points in time for prediction)
NLL (gaussian) is a bad idea, horribly overfits, even with orthograd (retry now that i am using returns)
SGD is just worse than Adam

Returns are the way to go!!! (much better than z normed prices)
finally beats persistence model (kind of an achievement?)
With returns, seems to learns everything within the first ~300 steps (makes sense, not really the most complicated dataset)

means and stds dont seem bad? to feed as additonal input features, as the acc went down, but it seems more stable?

need waaaay more data, overfits (starts to do it) in ~1000 steps
higher batch sizes are very good (faster)

ok, scratch that, apparently i just needed to reduce the seq_len, 
now its weirdly good and does not really overfit, so thats cool, ig

lower model dim seems to help 
lower head dim too, but better to reduce both at once

#########current flaws, that need fixing asap##########


TODOs

use more ochlv for trading strategy
nll for confidence measure
financial data inputs


# 1. Major Equity Market Indexes (Broad Market Sentiment & Benchmarks):
# ^GSPC (S&P 500): Tracks 500 large-cap U.S. stocks. Excellent for overall U.S. market health.
# ^IXIC (NASDAQ Composite): Tracks most stocks listed on the Nasdaq exchange, heavily tech-weighted. Good for tech sector sentiment.
# ^DJI (Dow Jones Industrial Average): Tracks 30 large, publicly-owned U.S. companies. More of a historical/popular index but still watched.
# ^RUT (Russell 2000): Tracks 2000 small-cap U.S. stocks. Indicator for smaller companies and broader economic health.
# International Indexes (if your target stocks have global exposure):
# ^FTSE (FTSE 100 - UK): Large-cap UK stocks.
# ^GDAXI (DAX - Germany): Major German stocks.
# ^N225 (Nikkei 225 - Japan): Major Japanese stocks.
# ^HSI (Hang Seng Index - Hong Kong): Major Hong Kong stocks.
# ^STOXX50E (EURO STOXX 50): Blue-chip stocks from Eurozone countries.
# 2. Volatility Indexes (Market Fear/Uncertainty):
# ^VIX (CBOE Volatility Index): Often called the "fear index," it measures market expectations of 30-day volatility based on S&P 500 index options. Highly influential.
# ^VVIX (CBOE VIX of VIX Index): Measures the volatility of the VIX itself. Can indicate nervousness about future volatility.
# ^VXN (CBOE Nasdaq 100 Volatility Index): Similar to VIX, but for the Nasdaq 100.
# 3. Interest Rates & Bonds (Cost of Capital, Economic Outlook):
# Treasury Yields (proxy for risk-free rates and economic expectations):
# ^TNX (CBOE Interest Rate 10 Year T Note): 10-Year U.S. Treasury yield. Very important.
# ^TYX (CBOE Interest Rate 30 Year T Bond): 30-Year U.S. Treasury yield.
# ^FVX (CBOE Interest Rate 5 Year T Note): 5-Year U.S. Treasury yield.
# ^IRX (CBOE Interest Rate 13 Week T Bill): 13-Week (3-Month) U.S. Treasury Bill yield.
# Yield Curve Spreads (often leading indicators of recessions): You'd calculate these yourself after fetching individual yields. Common ones:
# 10-Year minus 2-Year (^TNX - ^FVX or directly fetch 2-year if available)
# 10-Year minus 3-Month (^TNX - ^IRX)
# 4. Commodities (Inflation, Industrial Demand, Global Growth):
# Crude Oil:
# CL=F (Crude Oil Futures): WTI crude oil.
# BZ=F (Brent Crude Oil Futures): Brent crude oil.
# Gold:
# GC=F (Gold Futures): Often seen as a safe-haven asset and inflation hedge.
# Silver:
# SI=F (Silver Futures): Both an industrial metal and a precious metal.
# Copper:
# HG=F (Copper Futures): "Dr. Copper," often seen as an indicator of global economic health due to its industrial uses.
# Broad Commodity Index ETFs (easier than individual futures sometimes):
# DBC (Invesco DB Commodity Index Tracking Fund)
# GSG (iShares S&P GSCI Commodity-Indexed Trust)
# 5. Currencies (Global Capital Flows, Relative Economic Strength):
# U.S. Dollar Index:
# DX-Y.NYB (U.S. Dollar Index Futures): Measures the value of the USD against a basket of foreign currencies.
# Major Currency Pairs (relative to USD, or against each other if relevant):
# EURUSD=X (Euro to USD)
# JPY=X (USD to Japanese Yen) - Note: some are quoted as USD/Other, others Other/USD.
# GBPUSD=X (British Pound to USD)
# AUDUSD=X (Australian Dollar to USD)
# CNY=X (USD to Chinese Yuan Renminbi)
# 6. Sector-Specific ETFs (If your target stock is in a specific sector):
# Technology: XLK, VGT, QQQ (Nasdaq 100, very tech-heavy)
# Financials: XLF, VFH
# Healthcare: XLV, VHT
# Energy: XLE, VDE
# Consumer Discretionary: XLY, VCR
# Consumer Staples: XLP, VDC
# Industrials: XLI, VIS
# Utilities: XLU, VPU
# Real Estate: XLRE, VNQ
# Materials: XLB, VAW
# (Search for "(Sector Name) Sector SPDR ETF" or "Vanguard (Sector Name) ETF" to find tickers)
# Tips for Incorporating These:
# Relevance: Choose indicators most relevant to the stocks you are trying to predict. A tech stock will be more influenced by ^IXIC and ^VXN than an oil company.
# Lagged Features: Consider using lagged values of these indicators (e.g., the VIX from yesterday, or the average VIX over the last week) as input features. The market often reacts with a delay.
# Transformations: Just like your primary stock data, you'll likely want to use percentage changes or standardized values for these indicators rather than raw price levels to ensure stationarity and comparable scales.
# Feature Engineering: Create new features, like the yield curve spreads mentioned above.
# Data Availability & Alignment: yfinance is great, but be mindful of:
# Trading Hours: Different markets have different trading hours. Daily data (interval="1d") is usually the easiest to align.
# Holidays: Market holidays can lead to missing data points. pandas can help with ffill() or bfill() or interpolation if needed carefully.
# Start Dates: Ensure all chosen indicators have historical data covering your desired training period.
# Curse of Dimensionality: Don't add too many features without reason. Feature selection or dimensionality reduction techniques might be necessary if you include a very large number. Start with a core set and expand.
# Causality vs. Correlation: Remember that correlation doesn't imply causation. While these indicators can improve predictive power, they are part of a complex system.
# When adding these to your download_and_process_numerical_financial_data function, you'd fetch them alongside your primary tickers and then process their percentage changes and normalize them using the training set statistics of those specific indicators. They would become additional columns in your input_data_np.

# Selective Inclusion (Your Suggestion):
# "Maybe only they should be included for a select few input features" â€“ This is a very sensible approach if you were to revisit this.
# For instance, the per-sequence mean and std of the primary returns (your known_inputs) might be the most impactful if you were to choose any. These directly tell the model about the recent drift and volatility of what it's primarily trying to predict.
# However, given your past negative experience, it's wise to be cautious and perhaps focus on other avenues for improvement first.