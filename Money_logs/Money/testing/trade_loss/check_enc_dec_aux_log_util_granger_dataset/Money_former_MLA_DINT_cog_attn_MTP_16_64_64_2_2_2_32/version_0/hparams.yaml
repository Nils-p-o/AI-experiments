args: !!python/object:argparse.Namespace
  architecture: Money_former_MLA_DINT_cog_attn_MTP
  aux_input_features: 18
  aux_optimizer: adamw
  aux_tickers:
  - HG=F
  - QQQ
  - SI=F
  - ^IRX
  batch_size: 32
  bias: false
  class_weights:
  - - - 1.4649662971496582
      - 0.6357506513595581
      - 1.3432796001434326
    - - 1.1957405805587769
      - 0.7386548519134521
      - 1.2347418069839478
    - - 1.9145593643188477
      - 0.5291190147399902
      - 1.7013959884643555
    - - 1.7296642065048218
      - 0.5617762804031372
      - 1.5581539869308472
    - - 1.3389602899551392
      - 0.6885765194892883
      - 1.248625636100769
  - - - 1.6979272365570068
      - 0.5661681294441223
      - 1.5509001016616821
    - - 1.3209093809127808
      - 0.6526907086372375
      - 1.4068130254745483
    - - 2.349318265914917
      - 0.476040780544281
      - 2.1111111640930176
    - - 1.852799415588379
      - 0.5208463668823242
      - 1.8507407903671265
    - - 1.4236466884613037
      - 0.6311734318733215
      - 1.4020763635635376
  - - - 1.6442909240722656
      - 0.5982998013496399
      - 1.3880555629730225
    - - 1.408002257347107
      - 0.6771003007888794
      - 1.23018217086792
    - - 2.1575992107391357
      - 0.4949975311756134
      - 1.936821699142456
    - - 1.8507407903671265
      - 0.5383538007736206
      - 1.660684585571289
    - - 1.5006005764007568
      - 0.6544859409332275
      - 1.2411823272705078
  - - - 1.4471473693847656
      - 0.648624062538147
      - 1.3033385276794434
    - - 1.206130862236023
      - 0.760925829410553
      - 1.167250633239746
    - - 1.9390764236450195
      - 0.525778591632843
      - 1.7171821594238281
    - - 1.6346091032028198
      - 0.5745659470558167
      - 1.5437133312225342
    - - 1.286229133605957
      - 0.7204440832138062
      - 1.1983213424682617
  - - - 0.6837711930274963
      - 6.035024166107178
      - 0.728956937789917
    - - 0.6989789009094238
      - 6.123774528503418
      - 0.7112154960632324
    - - 0.6882920265197754
      - 6.743589878082275
      - 0.7148783802986145
    - - 0.6834906339645386
      - 6.381864547729492
      - 0.7245178818702698
    - - 0.6768251657485962
      - 6.6626667976379395
      - 0.7286381125450134
  classification_threshold: 0.01
  config: ./experiment_configs/granger_enc_dec_aux_trade_loss_exp_base.json
  d_ff: 64
  d_model: 64
  dataset: Money
  dec_layers: 2
  down_up_loss_weight: 1.0
  drawdown_lambda: 0.0
  dropout: 0.25
  dtype: fp32
  enc_layers: 2
  experiment_notes: trade loss - (transaction costs, exposure rebalance), log util
    loss, hhi penalty (over seq_len), just chlov returns, real estate granger dataset
    cluster nr.2, causal, no var or drawdown penalty, had small flaw in hhi and hhi
    penalty (forgot abs)
  extra_descriptor: ''
  flat_p_err_w: 1.5
  flat_t_err_w: 2.0
  folder_name: testing/trade_loss/check_enc_dec_aux_log_util_granger_dataset/
  head_dim: 32
  hhi_lambda: 0.0015
  include_sep_in_loss: false
  indices_to_predict:
  - 1
  - 2
  input_features: 15
  kv_compression_dim: 16
  lr: 0.00025
  lr_mult: 0.5
  muon_lr: 0.005
  n_noisy_copies: 0
  nhead: 2
  noise_factor: 0.05
  normalization_means:
  - - 0.000758264446631074
    - 0.0009282782557420433
    - 0.0005419229273684323
    - 0.00040947220986709
    - 0.00105004059150815
  - - 0.0006879241554997861
    - 0.0008515057852491736
    - 0.00047745564370416105
    - 0.00038741109892725945
    - 0.0009535165736451745
  - - 0.0008154697134159505
    - 0.0009706384153105319
    - 0.0004969580913893878
    - 0.00040555433952249587
    - 0.001071161706931889
  - - 0.0008058036328293383
    - 0.0009247622801922262
    - 0.00053230463527143
    - 0.0004184572899248451
    - 0.001097232336178422
  - - 0.11638286709785461
    - 0.20786862075328827
    - 0.1354581117630005
    - 0.18160082399845123
    - 0.1631070375442505
  normalization_stds:
  - - 0.028667086735367775
    - 0.040934007614851
    - 0.017888113856315613
    - 0.01781105250120163
    - 0.03750482201576233
  - - 0.026648610830307007
    - 0.0398668497800827
    - 0.013887986540794373
    - 0.016875166445970535
    - 0.03514806926250458
  - - 0.02996230497956276
    - 0.041050348430871964
    - 0.015091956593096256
    - 0.0173187218606472
    - 0.037553176283836365
  - - 0.030327606946229935
    - 0.040418002754449844
    - 0.017415503039956093
    - 0.018231941387057304
    - 0.03910258784890175
  - - 0.6777504682540894
    - 1.2441495656967163
    - 0.9575808644294739
    - 0.9881287813186646
    - 0.8388869762420654
  num_classes: 3
  num_params: 216080
  optimizer: muon
  prediction_type: portfolio
  q_compression_dim: 32
  qk_rope_dim: 16
  reload_data: true
  scheduler_type: cosine_restarts
  seed: 1
  seq_len: 16
  steepness_correction: 0.0
  t_0: 4000
  t_mult: 1.5
  t_total: 5000
  tickers:
  - AMT
  - EQIX
  - FRT
  - IRM
  - SBAC
  type: ''
  unique_inputs_ratio:
  - 0
  - 1
  up_down_err_w: 1.0
  use_global_seperator: true
  var_lambda: 0.0
  warmup_steps: 1000
  weight_decay: 0.0
batch_size: 32
learning_rate: 0.00025
lr_mult: 0.5
t_0: 5000
t_mult: 1.5
warmup_steps: 1000
